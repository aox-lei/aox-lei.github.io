title: 如何搭建稳定的代理ip池, 供爬虫使用
date: 2018-10-08
slug: python/proxy-ip-pool
Summary: 如何搭建稳定的代理ip池, 提供给爬虫来使用呢? 对于封ip的网站, 一般要不然花钱购买代理, 要不然就是抓取免费的代理ip, 但是使用过一段时间后都会发现, 免费的代理ip很不稳定, github上的代理ip池的project的star很高, 但是然并卵。那么如何解决这个问题呢?

新型的代理ip池[aox_proxy_pool](https://github.com/aox-lei/aox_proxy_pool)

在这篇文章之前, 应该不少人都看过很多搭建代理ip池的文章, 然后发现都是坑, 无法使用。说的比较多的
1. **推荐买xx家的代理ip, 贼稳定, 好使(广告)**
2. **抓取xx免费代理ip, 然后自己写一个校验, 然后写一个api, 提供给爬虫使用**

第一种方法就不说了, 真的存在几家不错的代理ip(别问我谁家的好, 我不知道, 我写这篇文章就为了推销我自己写的项目好伐)

第二种, github上这种项目海了去了, 绝大部分, 在你搭建好以后, 发现爬虫还是爬不动, 免费代理ip根本无法使用的好伐！稍微好点的haipproxy, 我也用过, 但是不解决根本问题! 绝大部分ip失效太快了! 而且各种错误, 爬十个页面, 能成功一个都算烧高香了。

那么, 到底为啥免费的代理ip不好使呢?
还有很多人都问到, 那些代理ip商真的有那么多ip么？

其实不是, 免费代理ip很多都是扫出来的, 扫ip段, 端口, 特征码。发现可以使用, 那就是代理ip。

代理ip不好用, 一般是因为以下几个原因
1. 扫到的代理ip是临时的
2. 访问量太大, 服务器都挂了
3. 本来就不是代理ip
4. 有验证
5. 本来是http的代理, 你用来访问https, 那当然不行了!
6. 代理异常, 连接中断, 带宽被沾满, 返回错误。

如果不想花钱, 那么就只能自己找到稳定的代理ip, 然后来使用。
而一般的代理池, 都是拿百度、知乎阿、豆瓣阿啥的网址访问, 成了说明能用, 不成就是失败。最多加个分值计算什么的。

之前看了下haipproxy的代码, 成功率高就得自己写验证, 说白了, 在爬虫使用之前, 先尝试访问下, 来提高成功率, 我觉得意义不大。

**上面都废话, 以下才是主要的**
其实免费代理ip中, 有极少数的一部分, 是非常稳定的代理服务器, 所以这些服务器就可以长期用来使用。

我抓取到的免费的代理ip, 中, 过滤后剩下的ip的访问成功率基本在90%+

第一、其实最简单的方式就是根据服务器开放的端口来判断, 如果服务器有开放80, 3389, 3306, 22之类的端口, 那么说明服务器还有别的服务在运行, 挂掉的几率很小, 如果是政府、学校的服务器, 那么更加稳定。当然也有可能开放别的端口

第二、服务器的访问速度判断, 需要访问多个不同的网址, 来取平均数, 这样的访问速度才比较稳

第三、代理ip的存活时间, 越长越稳定, 当然这个是在你搭建抓取后, 来进行计算。

第四、代理类型的重新检测, 通过访问不同的http和https网站, 判断代理到底是http还是https, 并且进行划分, http的代理, 那就访问http网址的时候使用, https的代理给https访问提供服务, 这样访问的几率才能提高。

所以根据这几点, 我重新写了一套代理ip池的项目, 目前抓取ip 4500+, 长期稳定的ip在60+左右, 虽然少，但是相当稳定。